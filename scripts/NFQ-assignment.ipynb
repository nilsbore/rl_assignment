{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitted Q-learning in continuous state spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we consider an appliction of neural fitted Q-learning to a navigation problem on a grid. Our agent can move in 4 directions: up, down, left, right. The goal is a randomly picked cell of the grid. Instead of directly perceiving its position, the agent gets a 10-dimensional observation vector from the environment. Note that this way we deal with a continuous state space. Therefore, the goal of the assignment is to implement neural fitted Q-learning.\n",
    "\n",
    "Hint: use PCA to reduce the dimensionality of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import the necessary packages. We use PyBrain for learning, as it has the implementation of NFQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import zeros, clip, asarray\n",
    "from enum import IntEnum\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from pybrain.rl.environments.environment import Environment\n",
    "from pybrain.rl.environments import EpisodicTask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyBrain, A RL problem can be specified as a pair of classes -- the Task and the Environment. \n",
    "\n",
    "Task: This class implements the nature of the task, and focuses on the reward aspect. It decides what is a successful action, and what is a less successful one. \n",
    "Environment: This class manages the inputs (observations) and outputs (actions performed) that go to and come from the agent.\n",
    "\n",
    "We provide our own derivations of these classes to describe the behaviour of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action(IntEnum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class specifies the behaviour of the environment (the grid). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NavigationEnv(Environment):\n",
    "    def __init__(self, height, width, measurement_model):   \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.position = (int(self.width/2), int(self.height/2))\n",
    "        self.measurement_model = measurement_model\n",
    "\n",
    "    \n",
    "    def getSensors(self):\n",
    "        \"\"\" \n",
    "        the currently visible state of the world \n",
    "            \n",
    "        \"\"\" \n",
    "        observation = np.dot(self.measurement_model, np.array(self.position))        \n",
    "        return  observation\n",
    "        \n",
    "        \n",
    "    \n",
    "    def getPosition(self):\n",
    "        return  np.array(self.position)                \n",
    "                    \n",
    "                    \n",
    "    def performAction(self, action):\n",
    "        \"\"\" \n",
    "        perform an action on the world that changes it's internal state (makes a move)\n",
    "            \n",
    "        \"\"\"\n",
    "        #moving the agent with respect to the action \n",
    "        if action == Action.UP:\n",
    "            self.position = (self.position[0], self.position[1] - 1)\n",
    "        elif action == Action.DOWN:\n",
    "            self.position = (self.position[0], self.position[1] + 1)\n",
    "        elif action == Action.LEFT:\n",
    "            self.position = (self.position[0] - 1, self.position[1])\n",
    "        elif action == Action.RIGHT:\n",
    "            self.position = (self.position[0] + 1, self.position[1])\n",
    "   \n",
    "        #if the agent has reached the boundary of the grid, it cannot move further    \n",
    "        if self.position[0] < 0:\n",
    "            self.position = (0, self.position[1])\n",
    "        elif self.position[1] < 0:\n",
    "            self.position = (self.position[0], 0)\n",
    "        elif self.position[0] >= self.width:\n",
    "            self.position = (self.width-1, self.position[1])\n",
    "        elif self.position[1] >= self.height:\n",
    "            self.position = (self.position[0], self.height-1)\n",
    "                 \n",
    "    def reset(self):\n",
    "        self.position = (int(self.width/2), int(self.height/2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class establishes the relationship between the agent and the environment -- i.e., specifies the task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NavigationTask(EpisodicTask):\n",
    "    def __init__(self, environment, goal):\n",
    "        self.env = environment\n",
    "        self.goal = goal\n",
    "        self.t = 0\n",
    "        self.episode_length = 250\n",
    "\n",
    "    def performAction(self, action):\n",
    "    #updates the timer and changes the position of the agent with respect to the action\n",
    "        self.t += 1       \n",
    "        self.env.performAction(action)\n",
    "        \n",
    "    def getObservation(self):\n",
    "    #gets an observation from the environment\n",
    "    #(you can modify this method if you wish to reduce the dimensionality of the observations)\n",
    "        sensors = self.env.getSensors()\n",
    "        return sensors\n",
    "    \n",
    "    def getReward(self):\n",
    "    #gives 10000 points if the goal is reached and -1 otherwise\n",
    "        position = self.env.getPosition() \n",
    "        if position[0] == self.goal[0] and position[1] == self.goal[1]:\n",
    "            reward = 1000\n",
    "        else:\n",
    "            reward = -1\n",
    "        return reward\n",
    " \n",
    "        \n",
    "    def reset(self):    \n",
    "    #resets the timer and puts the agent to the initial position\n",
    "        self.env.reset()\n",
    "        self.t = 0 \n",
    "        \n",
    "        \n",
    "    def isFinished(self):\n",
    "    #this is method should return True when the episode (a walk in the grid) is finished -- i.e., when the goal is reached or the time is out\n",
    "\n",
    "        position = self.env.getPosition()\n",
    "        if position[0] == self.goal[0] and position[1] == self.goal[1]:\n",
    "            return True\n",
    "        if self.t >= self.episode_length:\n",
    "            return True\n",
    "            \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize our problem and create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set the size of the grid\n",
    "width = 10\n",
    "height = 10\n",
    "#set the dimensionality of the distorted observations\n",
    "measurement_dimension = 10\n",
    "#the distortion model -- a random vector\n",
    "measurement_model = np.random.rand(measurement_dimension, 2)\n",
    "#create the environment\n",
    "env = NavigationEnv(height, width, measurement_model)\n",
    "#set the goal\n",
    "goal = (random.randint(0, width-1), random.randint(0, height-1))\n",
    "#initialize the task\n",
    "task = NavigationTask(env, goal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "As an example of a function approximator, we provide you with an implementation of a linear regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegressor(object):\n",
    "\n",
    "    def __init__(self, measurement_dimension):\n",
    "\n",
    "        self.p = np.zeros((4, measurement_dimension+1))\n",
    "    \n",
    "    def fit(self, measurements, actions, total_rewards):\n",
    "\n",
    "        for i in range(0, 4):\n",
    "            inds = np.where(actions == i)[0]\n",
    "            N = len(inds)\n",
    "            A = np.hstack((measurements[inds], np.ones((N, 1))))\n",
    "            b = total_rewards[inds]\n",
    "            self.p[i] = np.linalg.lstsq(A, b)[0]\n",
    "\n",
    "    def predict(self, measurement):\n",
    "\n",
    "        predicted_rewards = np.dot(self.p, np.append(measurement, 1.0))\n",
    "        return predicted_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that done, we can now apply fitted Q-learning with a linear regressor. \n",
    "Below we perform 100 episodes, where each epsiode consists of a random walk (consisting of no more than N steps). \n",
    "If the goal is reached, we interrupt the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward history:\n",
      "[942, -250, 845, 891, -250, -250, 778, -250, 865, 973, 812, 806, 875, -250, -250, 897, 821, 896, 865, 963, 784, 955, -250, 986, 933, 927, 974, 992, 978, 987, 987, 992, 990, 992, 990, 986, 990, 988, 988, 990, 992, 990, 990, 992, 990, 988, 992, 988, 992, 990, 990, 992, 992, 990, 990, 992, 990, 983, 945, 963, 955, 939, 990, 986, 992, 990, 976, 976, 978, 937, 980, 968, 908, 979, 919, 992, 992, 977, 979, 982, 961, 980, 972, 974, 899, 926, 992, 925, 987, 955, 932, 986, 984, 968, 988, 951, 974, 979, 988, 974]\n",
      "Mean reward:\n",
      "873.09\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1 # the epsilon value for greedy exploration\n",
    "measurements = None # this will contain all of our measurements\n",
    "actions = None # this will contain all of our actions\n",
    "total_rewards = None # this will contain all of our future rewards\n",
    "regressor = LinearRegressor(measurement_dimension)\n",
    "reward_history = []\n",
    "\n",
    "for n in range(0, 100):\n",
    "   run_measurements = []\n",
    "   run_actions = []\n",
    "   total_reward = 0\n",
    "   # just do a random walk\n",
    "   while True:\n",
    "       u = random.random()\n",
    "       measurement = env.getSensors()\n",
    "       if u > epsilon and n > 5:\n",
    "          predicted_rewards = regressor.predict(measurement)\n",
    "          action = Action(np.argmax(predicted_rewards))\n",
    "       else:\n",
    "          action = Action(random.randint(0, 3))\n",
    "       task.performAction(action)\n",
    "       reward = task.getReward()\n",
    "       total_reward += reward\n",
    "       run_actions.append(int(action))  \n",
    "       run_measurements.append(measurement)\n",
    "       if task.isFinished():\n",
    "          task.reset()\n",
    "          break\n",
    "\n",
    "   episode_len = len(run_measurements)\n",
    "\n",
    "   if measurements is None:\n",
    "      measurements = np.vstack(run_measurements)\n",
    "      actions = np.array(run_actions, dtype=int)\n",
    "      total_rewards = total_reward*np.ones((episode_len,))\n",
    "   else:\n",
    "      measurements = np.concatenate((measurements, np.vstack(run_measurements)), 0)\n",
    "      actions = np.concatenate((actions, np.array(run_actions, dtype=int)), 0)\n",
    "      total_rewards = np.concatenate((total_rewards, total_reward*np.ones((episode_len,))), 0)\n",
    "\n",
    "   reward_history.append(total_reward)\n",
    "   # here, we should update our estimates of the q-values\n",
    "   regressor.fit(measurements, actions, total_rewards)\n",
    "\n",
    "print(\"Reward history:\")\n",
    "print(reward_history)\n",
    "print(\"Mean reward:\")\n",
    "print(np.mean(reward_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to apply neural fitted Q-learning to the problem. \n",
    "\n",
    "Hint 1: use PyBrain -- it has all you need! Class ActionValueNetwork implements a neural network, and class NFQ implements the algorithm itself.\n",
    "\n",
    "Hint 2: You might want to make the episodes longer to give your network more experience. For that, you can modify \n",
    "the value episode_length of the NavigationTask class\n",
    "\n",
    "Hint 3: You might also want to make your task simpler by reducing the dimensionality of your measurements (e.g., use PCA). For that, you can possibly modify the getObservation() method of the NavigationTask class.\n",
    "\n",
    "Hint 4: Finally, try to play around with the reward function -- method getReward of the NavigationTask class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
